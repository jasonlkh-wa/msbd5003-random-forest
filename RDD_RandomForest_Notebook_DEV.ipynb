{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bdfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import random as rand\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deb03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/27 17:11:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"randomforest\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3a5ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_categorical_arr: x_all_categorical_arr.shape=(1000, 10)\n",
      "all_numerical_arr: x_all_numerical_arr.shape=(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "#random gen dataset\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "x_all_categorical_arr = np.random.randint(0, 2, (1000, 10))\n",
    "x_all_numerical_arr = np.random.rand(1000, 10)\n",
    "y_categorical_arr = np.random.randint(0, 2, 1000)\n",
    "#balanced_arr = np.concatenate([balanced_categorical_arr, balanced_numerical_arr], axis=1)\n",
    "print(f\"all_categorical_arr: {x_all_categorical_arr.shape=}\")\n",
    "print(f\"all_numerical_arr: {x_all_numerical_arr.shape=}\")\n",
    "#print(f\"balanced_arr: {balanced_arr.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67df864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = spark.createDataFrame(x_all_numerical_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3928a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = spark.createDataFrame(y_categorical_arr,['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51014500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index\n",
    "x_indexed=x_data.withColumn(\"id\",monotonically_increasing_id())\n",
    "y_indexed=y_data.withColumn(\"id\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef2fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10']\n",
      "['y']\n"
     ]
    }
   ],
   "source": [
    "# DEVELOPMENT: create joined df for computation with one id\n",
    "joined_df = x_indexed.join(y_indexed, \"id\").drop('id')\n",
    "x_train = joined_df.drop('y')\n",
    "y_train = joined_df.select('y')\n",
    "print(x_train.columns)\n",
    "print(y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ac62236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bootstrap function definition\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# weighted bootstrap subdataset\n",
    "\n",
    "#partition the dataframe dataset\n",
    "joined_df = joined_df.repartition(10)\n",
    "joined_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a565f10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://192.168.0.197:4040'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77d68bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entropy for classification evaluation crtieria\n",
    "# receives a probably as input to calculate entropy\n",
    "\n",
    "def class_entropy(df,count_val=0, sum_val=0):\n",
    "    #  entropy calculation for binary classification\n",
    "    if count_val==0 or sum_val==0 or count_val == 0:\n",
    "        counts = df.rdd.map(lambda x: (x[0],1)).reduceByKey(lambda x,y: x+y)\n",
    "        sum_y = df.rdd.map(lambda x: (1,x[0])).reduceByKey(lambda x,y: x+y).collect()\n",
    "        totals = counts.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x+y).collect()\n",
    "    \n",
    "    else:\n",
    "        sum_y = [(1,sum_val)]\n",
    "        totals = [(1,count_val)]\n",
    "\n",
    "    #check if empty\n",
    "    if len(totals)==0 or len(sum_y) == 0:\n",
    "        total = 0\n",
    "        prob_1 = 0\n",
    "        prob_0 = 0\n",
    "        y = 0\n",
    "        \n",
    "    #calculate probabilities\n",
    "    if len(sum_y) > 0 and len(totals)> 0:\n",
    "        total = totals[0][1]\n",
    "        y = sum_y[0][1]\n",
    "        prob_1 = y/total\n",
    "        prob_0 = 1-prob_1\n",
    "\n",
    "    #calculate entropy\n",
    "    if prob_1 == 0 or prob_0 ==0:\n",
    "        entropy = 0\n",
    "    else: \n",
    "        entropy = -prob_1 * np.log2(prob_1) - prob_0 * np.log2(prob_0)\n",
    "    \n",
    "    return entropy, total, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "b20dd7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_a = joined_df.select('y').rdd.map(lambda x: (x[0],1)).reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "754001a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_y = joined_df.select('y').rdd.map(lambda x: (1,x[0])).reduceByKey(lambda x,y: x+y).collect()[0][1]\n",
    "totals = test.rdd.map(lambda x: (1,x[1])).reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a9c67af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9993506898146103, 1000, 485)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_entropy(joined_df.select('y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "669ccae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4689955935892812, 500, 50)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_entropy(joined_df.select('y'),count_val = 500, sum_val= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c63380d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tree from splitting\n",
    "\n",
    "# each tree\n",
    "# (i) for each feature: find_split\n",
    "# (ii) Mapbypartition(find_split)\n",
    "\n",
    "def feature_split(dataset, feature_array):\n",
    "    '''\n",
    "    Input: \n",
    "    partition: a pyspark dataframe partition to be called\n",
    "    feature_array: a feature array for the tree that is intiialized earlier on\n",
    "    '''\n",
    "    joined_df.select('y')\n",
    "    #define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "        StructField(\"left_count\", IntegerType(), True),\n",
    "        StructField(\"left_sum\", IntegerType(), True)\n",
    "        \n",
    "    ])\n",
    "    feature_df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    # for each feature array, get a split and append the dataframe \n",
    "    for feature_index in feature_array:\n",
    "        \n",
    "        # find split\n",
    "        result_df = new_split_2(dataset, feature_index)\n",
    "        \n",
    "        #add feature  \n",
    "        feature_df = feature_df.union(result_df)\n",
    "        \n",
    "    return feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "ad1f92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "It \n",
    "#to delete\n",
    "parent_entropy, total_count = class_entropy(joined_df.select('y'))\n",
    "global broadcast_parent_entropy \n",
    "broadcast_parent_entropy = spark.sparkContext.broadcast(parent_entropy)\n",
    "global broadcast_parent_count \n",
    "broadcast_parent_count = spark.sparkContext.broadcast(total_count)\n",
    "\n",
    "result_df, info_gain, distinct_values, joined  = new_split_2(joined_df, 0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "58c5c2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------+-------------------+------------------+--------------------+\n",
      "|        split_value|left_count|left_sum|          left_prob|      left_entropy|           info_gain|\n",
      "+-------------------+----------+--------+-------------------+------------------+--------------------+\n",
      "|0.23754413087432746|        31|      15| 0.4838709677419355|0.9992492479956565|  0.6899441748741564|\n",
      "| 0.2310747965880714|        29|      14| 0.4827586206896552|0.9991421039919088|  0.7099602315951563|\n",
      "| 0.9629920038589946|        97|      49| 0.5051546391752577|0.9999233329473267|0.029785808793903024|\n",
      "| 0.7234201136885849|        71|      36| 0.5070422535211268|0.9998568991526107| 0.28981304335445635|\n",
      "| 0.5670162609866678|        54|      27|                0.5|               1.0| 0.45971144175280987|\n",
      "| 0.9699120461072704|        98|      49|                0.5|               1.0| 0.01971144175280992|\n",
      "|0.26899340443509767|        34|      18| 0.5294117647058824|0.9975025463691153|  0.6605605759873107|\n",
      "| 0.7072386343133986|        67|      34| 0.5074626865671642|0.9998393017810485| 0.32981910955950733|\n",
      "| 0.7582631959290226|        72|      36|                0.5|               1.0| 0.27971144175280993|\n",
      "| 0.7965372909761763|        76|      38|                0.5|               1.0|  0.2397114417528099|\n",
      "| 0.9383404568210378|        93|      47| 0.5053763440860215|0.9999165959739289| 0.06978900749705597|\n",
      "| 0.1533514031160802|        17|       8|0.47058823529411764|0.9975025463691153|  0.8301360088700602|\n",
      "|0.13882717264941014|        14|       8| 0.5714285714285714|0.9852281360342516|  0.8617795027080146|\n",
      "|0.17337359529475482|        22|      12| 0.5454545454545454|0.9940302114769565|  0.7810247952278795|\n",
      "|0.08011484638467514|         9|       6| 0.6666666666666666|0.9182958340544896|  0.9170648166879058|\n",
      "|0.06707647738842748|         7|       5| 0.7142857142857143| 0.863120568566631|  0.9392930019531457|\n",
      "|0.02007119777772637|         2|       2|                1.0|              NULL|                NULL|\n",
      "+-------------------+----------+--------+-------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_gain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "1969d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06707647442817688\n"
     ]
    }
   ],
   "source": [
    "split = result_df.rdd.collect()[0][1]\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "873751ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.select('_1','y').where(col('_1') <= split).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "2c01f449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------+-------------------+------------------+--------------------+\n",
      "|        split_value|left_count|left_sum|          left_prob|      left_entropy|           info_gain|\n",
      "+-------------------+----------+--------+-------------------+------------------+--------------------+\n",
      "|0.23754413087432746|        31|      15| 0.4838709677419355|0.9992492479956565|  0.6899441748741564|\n",
      "| 0.2310747965880714|        29|      14| 0.4827586206896552|0.9991421039919088|  0.7099602315951563|\n",
      "| 0.9629920038589946|        97|      49| 0.5051546391752577|0.9999233329473267|0.029785808793903024|\n",
      "| 0.7234201136885849|        71|      36| 0.5070422535211268|0.9998568991526107| 0.28981304335445635|\n",
      "| 0.5670162609866678|        54|      27|                0.5|               1.0| 0.45971144175280987|\n",
      "| 0.9699120461072704|        98|      49|                0.5|               1.0| 0.01971144175280992|\n",
      "|0.26899340443509767|        34|      18| 0.5294117647058824|0.9975025463691153|  0.6605605759873107|\n",
      "| 0.7072386343133986|        67|      34| 0.5074626865671642|0.9998393017810485| 0.32981910955950733|\n",
      "| 0.7582631959290226|        72|      36|                0.5|               1.0| 0.27971144175280993|\n",
      "| 0.7965372909761763|        76|      38|                0.5|               1.0|  0.2397114417528099|\n",
      "| 0.9383404568210378|        93|      47| 0.5053763440860215|0.9999165959739289| 0.06978900749705597|\n",
      "| 0.1533514031160802|        17|       8|0.47058823529411764|0.9975025463691153|  0.8301360088700602|\n",
      "|0.13882717264941014|        14|       8| 0.5714285714285714|0.9852281360342516|  0.8617795027080146|\n",
      "|0.17337359529475482|        22|      12| 0.5454545454545454|0.9940302114769565|  0.7810247952278795|\n",
      "|0.08011484638467514|         9|       6| 0.6666666666666666|0.9182958340544896|  0.9170648166879058|\n",
      "|0.06707647738842748|         7|       5| 0.7142857142857143| 0.863120568566631|  0.9392930019531457|\n",
      "|0.02007119777772637|         2|       2|                1.0|              NULL|                NULL|\n",
      "+-------------------+----------+--------+-------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "info_gain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "5d6c616a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "c309db3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|sum(left_count)|\n",
      "+---------------+\n",
      "|            793|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca6f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00afa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10313249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_split_2(joined_df, feature_index):\n",
    "    \n",
    "    # Select relevant columns\n",
    "    feature_col_name = joined_df.columns[feature_index]\n",
    "    y_col_name = joined_df.columns[-1]\n",
    "    split_data = joined_df.select(feature_col_name, y_col_name)\\\n",
    "        .withColumnRenamed(feature_col_name,\"feature\")\\\n",
    "        .withColumnRenamed(y_col_name,\"y\")\n",
    "    \n",
    "    \n",
    "    # Sample distinct values for splitting\n",
    "    distinct_values = split_data.select(\"feature\")\\\n",
    "        .withColumnRenamed(\"feature\",\"split_value\")\\\n",
    "        .distinct()\\\n",
    "        .sample(False, 0.1)\n",
    "    \n",
    "    #broadcast join\n",
    "    joined = split_data.join(distinct_values.hint('broadcast'),\\\n",
    "                             split_data[\"feature\"] <= distinct_values[\"split_value\"], \"inner\")\n",
    "\n",
    "    # Calculate conditional entropy \n",
    "    conditional_counts = joined.groupBy(\"split_value\").agg(\n",
    "        F.count(\"y\").alias(\"left_count\"),\n",
    "        F.sum(\"y\").alias(\"left_sum\")\n",
    "    )\n",
    "      \n",
    "    # Calculate probabilities and entropy\n",
    "    conditional_counts = conditional_counts.withColumn(\"left_prob\", F.col(\"left_sum\") / F.col(\"left_count\"))\n",
    "    conditional_counts = conditional_counts.withColumn(\n",
    "        \"left_entropy\",\n",
    "        -F.col(\"left_prob\") * F.log2(F.col(\"left_prob\"))\\\n",
    "        - (1 - F.col(\"left_prob\")) * F.log2(1 - F.col(\"left_prob\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    # Calculate Information Gain\n",
    "    info_gain = conditional_counts.withColumn(\n",
    "        \"info_gain\",\n",
    "        broadcast_parent_entropy.value - (F.col(\"left_entropy\") * (F.col(\"left_count\") / broadcast_parent_count.value))\n",
    "    )\n",
    "    \n",
    "    # Get the best split\n",
    "    best_split = info_gain.orderBy(F.desc(\"info_gain\")).first()\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "        StructField(\"left_count\", IntegerType(), True),\n",
    "        StructField(\"left_sum\", IntegerType(), True)\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    # Prepare output DataFrame\n",
    "    if best_split is None or best_split[\"info_gain\"] is None or best_split[\"split_value\"] is None:\n",
    "        result_df = spark.createDataFrame([(feature_index, float(0), float(0),int(0),int(0))], schema)\n",
    "    else:\n",
    "        result_df = spark.createDataFrame([(feature_index, \\\n",
    "                                            float(best_split[\"split_value\"]), \\\n",
    "                                            best_split[\"info_gain\"], \\\n",
    "                                            info_gain.select(\"left_count\").first()[0], \\\n",
    "                                            info_gain.select('left_sum').first()[0])], schema)\n",
    "    \n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f903dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node_2(df,feature_array):\n",
    "\n",
    "    #init\n",
    "    y_label = df.columns[-1]\n",
    "    node_parent = {}\n",
    "    best_gain = -1\n",
    "    best_feature= 0 \n",
    "    best_split = 0\n",
    "    \n",
    "    #get first tree\n",
    "    feature_df = feature_split(df,feature_array)\n",
    "    \n",
    "    #find best\n",
    "    best_split_info = feature_df.orderBy(col(\"info_gain\").desc()).first()\n",
    "    \n",
    "    if best_split_info is not None:\n",
    "        best_feature = best_split_info[0]\n",
    "        best_split = best_split_info[1]\n",
    "        best_gain = best_split_info[2] if best_split_info[2] is not None else 0\n",
    "        left_count = best_split_info[3]\n",
    "        left_sum = best_split_info[4]\n",
    "    \n",
    "    #generate split\n",
    "    left_df = df.filter(col(df.columns[best_feature]) <= best_split)\n",
    "    right_df = df.filter(col(df.columns[best_feature]) > best_split)\n",
    "        \n",
    "    #assign dictinary value of; key: feature, split_value, gain, and child\n",
    "    node_parent['feature'] = best_feature\n",
    "    node_parent['split_value'] = best_split\n",
    "    node_parent['gain'] = best_gain\n",
    "    node_parent['left_count'] = left_count\n",
    "    node_parent['left_sum'] = left_sum\n",
    "    \n",
    "    return node_parent, left_df, right_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "b3c57ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parent, left_df, right_df  = split_node_2(joined_df,[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "30110afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature': 0,\n",
       " 'split_value': 0.08279866725206375,\n",
       " 'gain': -0.08812908828258514,\n",
       " 'left_count': 74,\n",
       " 'left_sum': 37}"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd26f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e6754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a648d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead24a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42353a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree_bfs(df, feature_array, max_depth):\n",
    "    \"\"\" Build the decision tree using a breadth-first approach. \"\"\"\n",
    "    label_col = df.columns[-1]\n",
    "    global broadcast_parent_entropy\n",
    "    global broadcast_parent_count\n",
    "    global broad_left_entropy\n",
    "    global broad_left_count\n",
    "    global broad_right_entropy\n",
    "    global broad_right_count\n",
    "    global y_sum\n",
    "    \n",
    "    # Initialize a queue for BFS with root node\n",
    "    queue = deque([{\"node_df\": df, \"depth\": 0, \"path\": [],\"parent_count\":0,\"parent_entropy\":0,\"parent_ysum\":0}])\n",
    "    root = None\n",
    "    \n",
    "    \n",
    "    while queue:\n",
    "        # Get the front node of the queue\n",
    "        current = queue.popleft()\n",
    "        current_df = current[\"node_df\"]\n",
    "        current_depth = current[\"depth\"]\n",
    "        path = current[\"path\"]\n",
    "        \n",
    "        if current_depth==0:\n",
    "            #calculate first parent entropy\n",
    "            parent_entropy, parent_count, sum_y = class_entropy(df.select(label_col))\n",
    "            \n",
    "            #broadcast\n",
    "            broadcast_parent_entropy = spark.sparkContext.broadcast(parent_entropy)\n",
    "            broadcast_parent_count = spark.sparkContext.broadcast(parent_count)\n",
    "            y_sum = spark.sparkContext.broadcast(sum_y)\n",
    "        else:\n",
    "            #update entropies & counts\n",
    "            if path[-1] == \"right\":\n",
    "                parent_count = broadcast_total_count.value - node_parent['left_count']\n",
    "                y_count = y_sum.value - node_parent['left_sum']\n",
    "            else:\n",
    "                parent_count = node_parent['left_count']\n",
    "                y_count = node_parent['left_sum']\n",
    "            \n",
    "            #compute the parent_entropy\n",
    "            parent_entropy, parent_count, sum_y = class_entropy(current_df.select(label_col), \\\n",
    "                                                                      parent_count, \\\n",
    "                                                                      y_count)\n",
    "            #broadcast \n",
    "            broadcast_parent_entropy = spark.sparkContext.broadcast(parent_entropy)\n",
    "            broadcast_parent_count = spark.sparkContext.broadcast(parent_count)\n",
    "            y_sum = spark.sparkContext.broadcast(sum_y)\n",
    "        \n",
    "        \n",
    "        if current_df is None or current_df.rdd.isEmpty():\n",
    "            continue\n",
    "        \n",
    "        # Check for stopping conditions\n",
    "        #distinct_count = current_df.select(label_col).distinct().count()\n",
    "        if current_depth == max_depth:\n",
    "            most_common_label = current_df.groupBy(label_col).count().orderBy(\"count\", ascending=False).first()[label_col]\n",
    "            node = {\"label\": most_common_label}\n",
    "        else:\n",
    " \n",
    "            # Split the node\n",
    "            node_parent, left_df, right_df = split_node_2(current_df, feature_array)\n",
    "            node = {\n",
    "                \"feature\": node_parent['feature'], \n",
    "                \"threshold\": node_parent['split_value'], \n",
    "                \"left\": None, \n",
    "                \"right\": None\n",
    "            }\n",
    "            \n",
    "            # Enqueue children nodes\n",
    "            if left_df is not None and not left_df.rdd.isEmpty():\n",
    "                queue.append({\"node_df\": left_df,\\\n",
    "                              \"depth\": current_depth + 1,\\\n",
    "                              \"path\": path + [\"left\"],\\\n",
    "                              \"parent_count\": broadcast_parent_count.value,\\\n",
    "                              \"parent_entropy\": broadcast_parent_entropy.value,\\\n",
    "                              \"parent_ysum\":y_sum.value\\\n",
    "                             })\n",
    "                left_df.cache()\n",
    "            if right_df is not None and not right_df.rdd.isEmpty():\n",
    "                queue.append({\"node_df\": right_df, \"depth\": current_depth + 1, \"path\": path + [\"right\"]})\n",
    "                right_df.cache()\n",
    "        \n",
    "        # Set the node in the correct position in the tree\n",
    "        if not path:  # the root node\n",
    "            root = node\n",
    "        else:\n",
    "            parent = root\n",
    "            # Navigate to the correct parent node to attach the current node\n",
    "            for p in path[:-1]:\n",
    "                parent = parent[p]\n",
    "            parent[path[-1]] = node\n",
    "    \n",
    "    # Unpersist cached DataFrames\n",
    "    for item in queue:\n",
    "        if item[\"node_df\"] is not None:\n",
    "            item[\"node_df\"].unpersist()\n",
    "    \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca9885cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_train_bfs(df, num_trees, num_feature, max_depth):\n",
    "\n",
    "    #init\n",
    "    trees = {}\n",
    "    \n",
    "    # Pre-calculate the feature columns\n",
    "    columns = df.columns\n",
    "    feature_columns = df.columns[:-1]\n",
    "    column_size = len(columns)\n",
    "    \n",
    "    #set as indices list\n",
    "    column_indices = range(len(feature_columns))\n",
    "    feature_array = [*column_indices]\n",
    "\n",
    "    # def train_trees\n",
    "    def train_tree(df, column_size, feature_array, num_feature, max_depth):\n",
    "            \n",
    "        #sample feature\n",
    "        sampled = rand.sample(feature_array, num_feature)\n",
    "            \n",
    "        #add y column\n",
    "        sampled.append(column_size-1)\n",
    "\n",
    "        # Create a  DataFrame from the part of the partition\n",
    "        selected_columns = [columns[i] for i in sampled]\n",
    "        sampled_df = joined_df.select(selected_columns)\n",
    "            \n",
    "        # build information of new sample_trees\n",
    "        sample_df_columns = sampled_df.columns\n",
    "        sampled_df_features = sample_df_columns[:-1]\n",
    "        sampled_columns = range(len(sampled_df_features))\n",
    "        sampled_array = [*sampled_columns]\n",
    "        \n",
    "        tree = build_decision_tree_bfs(sampled_df, sampled_array, max_depth)\n",
    "            \n",
    "        return tree\n",
    "    \n",
    "    # Apply our function to each partition of the DataFrame\n",
    "    for tree_idx in range(num_trees):\n",
    "        tree = train_tree(df, column_size, feature_array, num_feature, max_depth)\n",
    "        trees[tree_idx] = tree\n",
    "        df.cache()\n",
    "\n",
    "\n",
    "    # Unpersist the DataFrame as it's no longer needed\n",
    "    df.unpersist()\n",
    "\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a0d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "219ca50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_train_dfs(df, num_trees, num_feature, max_depth):\n",
    "\n",
    "    #init\n",
    "    trees = {}\n",
    "    \n",
    "    # Pre-calculate the feature columns\n",
    "    columns = df.columns\n",
    "    feature_columns = df.columns[:-1]\n",
    "    column_size = len(columns)\n",
    "    \n",
    "    #set as indices list\n",
    "    column_indices = range(len(feature_columns))\n",
    "    feature_array = [*column_indices]\n",
    "\n",
    "    # def train_trees\n",
    "    def train_tree(df, column_size, feature_array, num_feature, max_depth):\n",
    "            \n",
    "        #sample feature\n",
    "        sampled = rand.sample(feature_array, num_feature)\n",
    "            \n",
    "        #add y column\n",
    "        sampled.append(column_size-1)\n",
    "\n",
    "        # Create a  DataFrame from the part of the partition\n",
    "        selected_columns = [columns[i] for i in sampled]\n",
    "        sampled_df = joined_df.select(selected_columns)\n",
    "            \n",
    "        # build information of new sample_trees\n",
    "        sample_df_columns = sampled_df.columns\n",
    "        sampled_df_features = sample_df_columns[:-1]\n",
    "        sampled_columns = range(len(sampled_df_features))\n",
    "        sampled_array = [*sampled_columns]\n",
    "        \n",
    "        tree = build_decision_tree(sampled_df, sampled_array, max_depth)\n",
    "            \n",
    "        return tree\n",
    "    \n",
    "    \n",
    "    # Apply our function to each partition of the DataFrame\n",
    "    for tree_idx in range(num_trees):\n",
    "        tree = train_tree(df, column_size, feature_array, num_feature, max_depth)\n",
    "        trees[tree_idx] = tree\n",
    "        df.cache()\n",
    "\n",
    "    # Unpersist the DataFrame as it's no longer needed\n",
    "    df.unpersist()\n",
    "\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82813153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree(df, feature_array, max_depth, current_depth=0):\n",
    "    \n",
    "    \"\"\" Recursively build the decision tree. \"\"\"\n",
    "    \n",
    "    #init\n",
    "    global broadcast_parent_entropy\n",
    "    global broadcast_parent_count\n",
    "    global broad_left_entropy\n",
    "    global broad_left_count\n",
    "    global broad_right_entropy\n",
    "    global broad_right_count\n",
    "    global y_sum\n",
    "    \n",
    "    #set y label\n",
    "    label_col = df.columns[-1]\n",
    "    #distinct_count = df.select(label_col).distinct().count()\n",
    "    \n",
    "    if df is None or df.rdd.isEmpty():\n",
    "        return df\n",
    "    elif current_depth == max_depth:\n",
    "        # Return the most common label\n",
    "        most_common_label = df.groupBy(label_col).count().orderBy(\"count\", ascending=False).first()[label_col]\n",
    "        return {\"label\": df.select(label_col).first()[0]}\n",
    "        #return most_common_label \n",
    "    #elif distinct_count == 1:\n",
    "    #   return {\"label\": df.select(label_col).first()[0]}\n",
    "    \n",
    "    #get first split entropy\n",
    "    if current_depth ==0:\n",
    "        parent_entropy, total_count,sum_y = class_entropy(df.select(label_col))\n",
    "        broadcast_parent_entropy = spark.sparkContext.broadcast(parent_entropy)\n",
    "        broadcast_parent_count = spark.sparkContext.broadcast(total_count)\n",
    "        y_sum = spark.sparkContext.broadcast(sum_y)\n",
    "    \n",
    "    #split node\n",
    "    node_parent, left_df, right_df = split_node_2(df,feature_array)\n",
    "    \n",
    "    # Cache potentially reused DataFrames\n",
    "    if left_df is not None and not left_df.rdd.isEmpty():\n",
    "        left_df.cache()\n",
    "    if right_df is not None and not right_df.rdd.isEmpty():\n",
    "        right_df.cache()\n",
    "\n",
    "    \n",
    "    #split if not empty    \n",
    "    if left_df is None or left_df == []:\n",
    "        #right\n",
    "        broad_right_count = broadcast_total_count.value - node_parent['left_count']\n",
    "        y_count = y_sum.value - node_parent['left_sum']\n",
    "        broadcast_parent_count = spark.sparkContext.broadcast(broad_right_count)\n",
    "        \n",
    "        #update entropies & counts\n",
    "        broad_right_entropy, broad_right_count, sum_y = class_entropy(right_df.select(label_col), \\\n",
    "                                                                      broadcast_total_count.value, \\\n",
    "                                                                      y_count)\n",
    "        broadcast_parent_entropy = spark.sparkContext.broadcast(broad_right_entropy)\n",
    "        y_sum = spark.sparkContext.broadcast(sum_y)\n",
    "        \n",
    "        #right tree\n",
    "        right_subtree = build_decision_tree(right_df, feature_array, max_depth, current_depth + 1)\n",
    "        \n",
    "    elif right_df is None or right_df ==[]:\n",
    "        \n",
    "        #left\n",
    "        broad_left_count = node_parent['left_count']\n",
    "        y_count = node_parent['left_sum']\n",
    "        broadcast_parent_count = spark.sparkContext.broadcast(broad_left_count)\n",
    "        \n",
    "        #update entropies & counts\n",
    "        broad_left_entropy, broad_left_count,sum_y = class_entropy(left_df.select(label_col),broadcast_total_count.value, y_count)\n",
    "        broadcast_parent_entropy = spark.sparkContext.broadcast(broad_left_entropy)\n",
    "        y_sum = spark.sparkContext.broadcast(sum_y)\n",
    "        \n",
    "        #left\n",
    "        left_subtree = build_decision_tree(left_df, feature_array, max_depth, current_depth + 1)\n",
    "        \n",
    "    elif (left_df is None) and (right_df is None):\n",
    "        return {\"feature\": node_parent['feature'] , \"threshold\": node_parent['split_value'], \"left\": None, \"right\": None}\n",
    "    else:\n",
    "        #left\n",
    "        broad_left_count = node_parent['left_count']\n",
    "        y_count = node_parent['left_sum']\n",
    "        broadcast_parent_count = spark.sparkContext.broadcast(broad_left_count)\n",
    "        \n",
    "        #update entropies & counts\n",
    "        broad_left_entropy, broad_left_count, sum_y = class_entropy(left_df.select(label_col),broadcast_total_count.value, y_count)\n",
    "        broadcast_parent_entropy = spark.sparkContext.broadcast(broad_left_entropy)\n",
    "        y_sum = spark.sparkContext.broadcast(sum_y)\n",
    "        \n",
    "        #left subtree\n",
    "        left_subtree = build_decision_tree(left_df, feature_array, max_depth, current_depth + 1)\n",
    "        \n",
    "        #right\n",
    "        broad_right_count = broadcast_total_count.value - node_parent['left_count']\n",
    "        y_count = y_sum.value - node_parent['left_sum']\n",
    "        broadcast_parent_count = spark.sparkContext.broadcast(broad_right_count)\n",
    "        \n",
    "        #update entropies & counts\n",
    "        broad_right_entropy, broad_right_count, sum_y = class_entropy(right_df.select(label_col), broadcast_total_count.value,y_sum.value)\n",
    "        broadcast_parent_entropy = spark.sparkContext.broadcast(broad_right_entropy)\n",
    "        y_sum = spark.sparkContext.broadcast(sum_y)\n",
    "        \n",
    "        right_subtree = build_decision_tree(right_df, feature_array, max_depth, current_depth + 1)\n",
    "\n",
    "    # Clear cache after use\n",
    "    if left_df is not None:\n",
    "        left_df.unpersist()\n",
    "    if right_df is not None:\n",
    "        right_df.unpersist()\n",
    "        \n",
    "    \n",
    "    # return node structure\n",
    "    return {\n",
    "        \"feature\": node_parent['feature'] , \n",
    "        \"threshold\": node_parent['split_value'], \n",
    "        \"left\": left_subtree, \n",
    "        \"right\": right_subtree\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f354832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "6a8283e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/0l2mg1j55w924ptz41_9t_2r0000gn/T/ipykernel_92044/3431669921.py:32: RuntimeWarning: invalid value encountered in log2\n",
      "  entropy = -prob_1 * np.log2(prob_1) - prob_0 * np.log2(prob_0)\n"
     ]
    }
   ],
   "source": [
    "dfs_tree = build_decision_tree(joined_df, [0,2,3], max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a826cd8",
   "metadata": {},
   "source": [
    "# Evaluation of dfs vs bfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3cf61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/0l2mg1j55w924ptz41_9t_2r0000gn/T/ipykernel_92044/3427659448.py:33: RuntimeWarning: invalid value encountered in log2\n",
      "  entropy = -prob_1 * np.log2(prob_1) - prob_0 * np.log2(prob_0)\n",
      "24/05/26 17:57:21 WARN CacheManager: Asked to cache already cached data.        \n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", False)\n",
    "\n",
    "#DFS approach \n",
    "dfs_start_PFirst = time.time()\n",
    "dfs_trees_PFirst = random_forest_train_dfs(joined_df, num_trees=3 , num_feature = 3, max_depth=5)\n",
    "dfs_end_PFirst = time.time()\n",
    "dfs_time_PFirst = dfs_end_PFirst - dfs_start_PFirst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs_time_PFirst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_trees_PFirst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "c6214602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/0l2mg1j55w924ptz41_9t_2r0000gn/T/ipykernel_92044/3427659448.py:33: RuntimeWarning: invalid value encountered in log2\n",
      "  entropy = -prob_1 * np.log2(prob_1) - prob_0 * np.log2(prob_0)\n",
      "24/05/26 17:50:42 WARN CacheManager: Asked to cache already cached data.        \n",
      "24/05/26 17:52:32 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.parallelismFirst\", False)\n",
    "\n",
    "#DFS approach \n",
    "dfs_start = time.time()\n",
    "dfs_trees = random_forest_train_dfs(joined_df, num_trees=3 , num_feature = 3, max_depth=5)\n",
    "dfs_end = time.time()\n",
    "dfs_time = dfs_end - dfs_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "1bbb7e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "371.3925850391388\n"
     ]
    }
   ],
   "source": [
    "print(dfs_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "dd2fe48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'feature': 2,\n",
       "  'threshold': 0.0018727439455688,\n",
       "  'left': {'feature': 0,\n",
       "   'threshold': 0.0,\n",
       "   'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "   'right': {'feature': 0,\n",
       "    'threshold': 0.0,\n",
       "    'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "     'right': {'feature': 2,\n",
       "      'threshold': 0.0018727439455688,\n",
       "      'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "      'right': DataFrame[_8: double, _1: double, _3: double, y: bigint]}}}},\n",
       "  'right': {'feature': 2,\n",
       "   'threshold': 0.03417558968067169,\n",
       "   'left': {'feature': 1,\n",
       "    'threshold': 0.17401044070720673,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "      'right': DataFrame[_8: double, _1: double, _3: double, y: bigint]}},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.7626534700393677,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "      'right': DataFrame[_8: double, _1: double, _3: double, y: bigint]},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.9815413951873779,\n",
       "      'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "      'right': DataFrame[_8: double, _1: double, _3: double, y: bigint]}}},\n",
       "   'right': {'feature': 0,\n",
       "    'threshold': 0.008619002997875214,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.003830290399491787,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.001203613937832415,\n",
       "      'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "      'right': DataFrame[_8: double, _1: double, _3: double, y: bigint]},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "      'right': DataFrame[_8: double, _1: double, _3: double, y: bigint]}},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.46038618683815,\n",
       "     'left': {'feature': 2,\n",
       "      'threshold': 0.040785014629364014,\n",
       "      'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "      'right': DataFrame[_8: double, _1: double, _3: double, y: bigint]},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.8140682578086853,\n",
       "      'left': DataFrame[_8: double, _1: double, _3: double, y: bigint],\n",
       "      'right': DataFrame[_8: double, _1: double, _3: double, y: bigint]}}}}},\n",
       " 1: {'feature': 2,\n",
       "  'threshold': 0.0035108509473502636,\n",
       "  'left': {'feature': 0,\n",
       "   'threshold': 0.0,\n",
       "   'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "   'right': {'feature': 0,\n",
       "    'threshold': 0.5821213722229004,\n",
       "    'left': {'feature': 2,\n",
       "     'threshold': 0.0035108509473502636,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]},\n",
       "     'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]},\n",
       "    'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]}},\n",
       "  'right': {'feature': 0,\n",
       "   'threshold': 0.6129773259162903,\n",
       "   'left': {'feature': 2,\n",
       "    'threshold': 0.02138884738087654,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.10044042021036148,\n",
       "     'left': {'feature': 2,\n",
       "      'threshold': 0.013949018903076649,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]}},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.19201624393463135,\n",
       "     'left': {'feature': 2,\n",
       "      'threshold': 0.044570039957761765,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.43558916449546814,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]}}},\n",
       "   'right': {'feature': 0,\n",
       "    'threshold': 0.639262855052948,\n",
       "    'left': {'feature': 2,\n",
       "     'threshold': 0.19333922863006592,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.616885781288147,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]}},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.7621296048164368,\n",
       "     'left': {'feature': 2,\n",
       "      'threshold': 0.011765094473958015,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.8733914494514465,\n",
       "      'left': DataFrame[_1: double, _5: double, _6: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _5: double, _6: double, y: bigint]}}}}},\n",
       " 2: {'feature': 0,\n",
       "  'threshold': 0.005803768988698721,\n",
       "  'left': {'feature': 0,\n",
       "   'threshold': 0.0,\n",
       "   'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "   'right': {'feature': 1,\n",
       "    'threshold': 0.23358044028282166,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "      'right': DataFrame[_6: double, _9: double, _7: double, y: bigint]}},\n",
       "    'right': {'label': 0}}},\n",
       "  'right': {'feature': 0,\n",
       "   'threshold': 0.01401451788842678,\n",
       "   'left': {'feature': 2,\n",
       "    'threshold': 0.5878856778144836,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "      'right': DataFrame[_6: double, _9: double, _7: double, y: bigint]}},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "     'right': {'feature': 2,\n",
       "      'threshold': 0.8982235193252563,\n",
       "      'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "      'right': DataFrame[_6: double, _9: double, _7: double, y: bigint]}}},\n",
       "   'right': {'feature': 2,\n",
       "    'threshold': 0.003929321654140949,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.7333279252052307,\n",
       "     'left': {'feature': 1,\n",
       "      'threshold': 0.7045860290527344,\n",
       "      'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "      'right': DataFrame[_6: double, _9: double, _7: double, y: bigint]},\n",
       "     'right': DataFrame[_6: double, _9: double, _7: double, y: bigint]},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.014856122434139252,\n",
       "     'left': {'label': 0},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.01835838332772255,\n",
       "      'left': DataFrame[_6: double, _9: double, _7: double, y: bigint],\n",
       "      'right': DataFrame[_6: double, _9: double, _7: double, y: bigint]}}}}}}"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show values\n",
    "dfs_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "9224e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#measuring time for one DFS tree\n",
    "dfs_tree_start = time.time()\n",
    "dfs_tree = build_decision_tree(joined_df, [0,2,3], max_depth=5)\n",
    "dfs_tree_end = time.time()\n",
    "dfs_tree_time = dfs_tree_end - dfs_tree_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "bd315711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.03761076927185\n"
     ]
    }
   ],
   "source": [
    "print(dfs_tree_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35c107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd91fd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/0l2mg1j55w924ptz41_9t_2r0000gn/T/ipykernel_7752/609407658.py:33: RuntimeWarning: invalid value encountered in log2\n",
      "  entropy = -prob_1 * np.log2(prob_1) - prob_0 * np.log2(prob_0)\n",
      "24/05/27 18:54:12 WARN CacheManager: Asked to cache already cached data.        \n",
      "24/05/27 18:54:17 WARN BlockManager: Block rdd_3025_8 already exists on this machine; not re-adding it\n",
      "24/05/27 18:55:11 WARN CacheManager: Asked to cache already cached data.        \n",
      "24/05/27 18:56:13 WARN CacheManager: Asked to cache already cached data.        \n"
     ]
    }
   ],
   "source": [
    "#BFS approach \n",
    "bfs_start = time.time()\n",
    "bfs_trees = random_forest_train_bfs(joined_df, num_trees=3 , num_feature = 3, max_depth=5)\n",
    "bfs_end = time.time()\n",
    "bfs_time = bfs_end - bfs_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "44c91fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268.3045129776001\n"
     ]
    }
   ],
   "source": [
    "print(bfs_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "99b96f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'feature': 1,\n",
       "  'threshold': 0.009205667302012444,\n",
       "  'left': {'feature': 0,\n",
       "   'threshold': 0.47529298067092896,\n",
       "   'left': {'feature': 0,\n",
       "    'threshold': 0.47529298067092896,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': None,\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}}},\n",
       "    'right': None},\n",
       "   'right': {'feature': 0,\n",
       "    'threshold': 0.0,\n",
       "    'left': None,\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': None,\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}}}}},\n",
       "  'right': {'feature': 0,\n",
       "   'threshold': 0.012003026902675629,\n",
       "   'left': {'feature': 0,\n",
       "    'threshold': 0.0,\n",
       "    'left': None,\n",
       "    'right': {'feature': 1,\n",
       "     'threshold': 0.9095953702926636,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 1}},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}}}},\n",
       "   'right': {'feature': 1,\n",
       "    'threshold': 0.010233856737613678,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.23713159561157227,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 1}}},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.015366250649094582,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.015482459217309952,\n",
       "      'left': {'label': 1},\n",
       "      'right': {'label': 0}}}}}},\n",
       " 1: {'feature': 1,\n",
       "  'threshold': 0.008354703895747662,\n",
       "  'left': {'feature': 0,\n",
       "   'threshold': 0.0,\n",
       "   'left': None,\n",
       "   'right': {'feature': 2,\n",
       "    'threshold': 0.8801313638687134,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': None,\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}}},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': None,\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}}}}},\n",
       "  'right': {'feature': 0,\n",
       "   'threshold': 0.010965916328132153,\n",
       "   'left': {'feature': 0,\n",
       "    'threshold': 0.0,\n",
       "    'left': None,\n",
       "    'right': {'feature': 2,\n",
       "     'threshold': 0.06382603198289871,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}},\n",
       "     'right': {'feature': 2,\n",
       "      'threshold': 0.27090921998023987,\n",
       "      'left': {'label': 1},\n",
       "      'right': {'label': 1}}}},\n",
       "   'right': {'feature': 1,\n",
       "    'threshold': 0.009827552363276482,\n",
       "    'left': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': None,\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}}},\n",
       "    'right': {'feature': 2,\n",
       "     'threshold': 0.013657836243510246,\n",
       "     'left': {'feature': 1,\n",
       "      'threshold': 0.6592445373535156,\n",
       "      'left': {'label': 0},\n",
       "      'right': {'label': 0}},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.015414958819746971,\n",
       "      'left': {'label': 1},\n",
       "      'right': {'label': 0}}}}}},\n",
       " 2: {'feature': 1,\n",
       "  'threshold': 0.004127154592424631,\n",
       "  'left': {'feature': 1,\n",
       "   'threshold': 0.003387847915291786,\n",
       "   'left': {'feature': 0,\n",
       "    'threshold': 0.0,\n",
       "    'left': None,\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': None,\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.3093242347240448,\n",
       "      'left': {'label': 1},\n",
       "      'right': None}}},\n",
       "   'right': {'feature': 0,\n",
       "    'threshold': 0.0,\n",
       "    'left': None,\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': None,\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}}}}},\n",
       "  'right': {'feature': 2,\n",
       "   'threshold': 0.004101450555026531,\n",
       "   'left': {'feature': 0,\n",
       "    'threshold': 0.0,\n",
       "    'left': None,\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.0,\n",
       "     'left': None,\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}}}},\n",
       "   'right': {'feature': 0,\n",
       "    'threshold': 0.004373982083052397,\n",
       "    'left': {'feature': 2,\n",
       "     'threshold': 0.5892717838287354,\n",
       "     'left': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 0}},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.0,\n",
       "      'left': None,\n",
       "      'right': {'label': 1}}},\n",
       "    'right': {'feature': 0,\n",
       "     'threshold': 0.009242314845323563,\n",
       "     'left': {'feature': 1,\n",
       "      'threshold': 0.5286298394203186,\n",
       "      'left': {'label': 1},\n",
       "      'right': {'label': 0}},\n",
       "     'right': {'feature': 0,\n",
       "      'threshold': 0.010513224638998508,\n",
       "      'left': {'label': 0},\n",
       "      'right': {'label': 0}}}}}}}"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfs_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40b17362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2d/0l2mg1j55w924ptz41_9t_2r0000gn/T/ipykernel_7752/609407658.py:33: RuntimeWarning: invalid value encountered in log2\n",
      "  entropy = -prob_1 * np.log2(prob_1) - prob_0 * np.log2(prob_0)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#measuring time for one BFS tree\n",
    "bfs_tree_start = time.time()\n",
    "bfs_tree = build_decision_tree_bfs(joined_df, [0,2,3], max_depth=5)\n",
    "bfs_tree_end = time.time()\n",
    "bfs_tree_time = bfs_tree_end - bfs_tree_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "785c7944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.81710481643677\n"
     ]
    }
   ],
   "source": [
    "print(bfs_tree_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "2c4bf2b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFS RF with Parallelism True:  192.8394010066986\n",
      "DFS RF with Parallliesm False:  161.65961813926697\n",
      "BFS RF:  135.3711497783661\n",
      "DFS Tree:  37.03761076927185\n",
      "BFS Tree:  33.81710481643677\n"
     ]
    }
   ],
   "source": [
    "print(\"DFS RF with Parallelism True: \", dfs_time)\n",
    "print(\"DFS RF with Parallliesm False: \", dfs_time_PFirst)\n",
    "print(\"BFS RF: \", bfs_time)\n",
    "print(\"DFS Tree: \", dfs_tree_time)\n",
    "print(\"BFS Tree: \", bfs_tree_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e852ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e4153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53acfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae070385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734d43d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
