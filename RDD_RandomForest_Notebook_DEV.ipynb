{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bdfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deb03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/04 22:28:00 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"randomforest\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3a5ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_categorical_arr: x_all_categorical_arr.shape=(100, 10)\n",
      "all_numerical_arr: x_all_numerical_arr.shape=(100, 10)\n"
     ]
    }
   ],
   "source": [
    "#random gen dataset\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "x_all_categorical_arr = np.random.randint(0, 2, (100, 10))\n",
    "x_all_numerical_arr = np.random.rand(100, 10)\n",
    "y_categorical_arr = np.random.randint(0, 2, 100)\n",
    "#balanced_arr = np.concatenate([balanced_categorical_arr, balanced_numerical_arr], axis=1)\n",
    "print(f\"all_categorical_arr: {x_all_categorical_arr.shape=}\")\n",
    "print(f\"all_numerical_arr: {x_all_numerical_arr.shape=}\")\n",
    "#print(f\"balanced_arr: {balanced_arr.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67df864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = spark.createDataFrame(x_all_numerical_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3928a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = spark.createDataFrame(y_categorical_arr,['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51014500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index\n",
    "x_indexed=x_data.withColumn(\"id\",monotonically_increasing_id())\n",
    "y_indexed=y_data.withColumn(\"id\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef2fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10']\n",
      "['y']\n"
     ]
    }
   ],
   "source": [
    "# DEVELOPMENT: create joined df for computation with one id\n",
    "joined_df = x_indexed.join(y_indexed, \"id\").drop('id')\n",
    "x_train = joined_df.drop('y')\n",
    "y_train = joined_df.select('y')\n",
    "print(x_train.columns)\n",
    "print(y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac62236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap function definition\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# weighted bootstrap subdataset\n",
    "\n",
    "#partition the dataframe dataset\n",
    "joined_df = joined_df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d68bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entropy for classification evaluation crtieria\n",
    "# receives a probably as input to calculate entropy\n",
    "\n",
    "def class_entropy(df):\n",
    "    #  entropy calculation for binary classification\n",
    "    col_name = \"y\"\n",
    "    counts = df.groupBy(col_name).count()\n",
    "    total = df.count()\n",
    "    return counts.withColumn(\"prob\", F.col(\"count\") / total).select(\n",
    "        F.sum(-F.col(\"prob\") * F.log2(F.col(\"prob\"))).alias(\"entropy\")\n",
    "    ).first()[\"entropy\"]\n",
    "\n",
    "def prob(df):\n",
    "    #  probably calcluation for binary classification\n",
    "    count = np.count_nonzero(df)   \n",
    "    if count == 0:\n",
    "        return float(0)\n",
    "    else:\n",
    "        total = len(df)\n",
    "        prob = np.divide(count,total)\n",
    "        return float(prob)\n",
    "\n",
    "class_entropy_udf = udf(class_entropy, ArrayType(DoubleType()))\n",
    "prob_udf = udf(prob,FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fd7d877",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:38\u001b[0;36m\u001b[0m\n\u001b[0;31m    (parent_entropy - F.sum(F.col(\"entropy\") * (F.col(\"count\") / parent_data_count))).alias(\"info_gain\")\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def new_split(joined_df, feature_index):\n",
    "    \n",
    "    # Select relevant columns\n",
    "    feature_col_name = joined_df.columns[feature_index]\n",
    "    y_col_name = joined_df.columns[-1]\n",
    "    split_data = joined_df.select(feature_col_name, y_col_name)\\\n",
    "        .withColumnRenamed(feature_col_name,\"feature\")\\\n",
    "        .withColumnRenamed(y_col_name,\"y\")\n",
    "    \n",
    "    # Calculate parent entropy\n",
    "    parent_entropy = class_entropy(joined_df.select(\"y\"))\n",
    "    parent_data_count = joined_df.count()\n",
    "    \n",
    "    # Calculate potential splits and their Information Gain\n",
    "    distinct_values = split_data.select(\"feature\")\\\n",
    "        .withColumnRenamed(\"feature\",\"split_value\")\\\n",
    "        .distinct().orderBy(\"split_value\")\\\n",
    "        .sample(False, min(1.0, 10/parent_data_count))\n",
    "    \n",
    "    # Cartesian join to get split mask\n",
    "    splits_info = distinct_values.crossJoin(split_data)\\\n",
    "        .withColumn(\n",
    "        \"is_left\", F.col(\"feature\") <= F.col(\"split_value\")\n",
    "    )\n",
    "    \n",
    "    #aggregate list\n",
    "    entropies = splits_info.groupBy(\"split_value\", \"is_left\").agg(\n",
    "        F.count(\"y\").alias(\"count\"),\n",
    "        F.sum(\"y\").alias(\"sum\"),\n",
    "        prob_udf(F.collect_list(\"y\")).alias(\"prob\")\n",
    "    )\n",
    "    entropies = entropies.withColumn(\"entropy\",\\\n",
    "                                    -F.col(\"prob\") * F.log2(F.col(\"prob\")) \\\n",
    "                                    -(1-F.col(\"prob\")) * F.log2((1-F.col(\"prob\")))\n",
    "                                    )\n",
    "    # Calculate Information Gain for each split\n",
    "    info_gain = entropies.groupBy(\"split_value\").agg(\n",
    "        (parent_entropy - F.sum(F.col(\"entropy\") * (F.col(\"count\") / parent_data_count))).alias(\"info_gain\")\n",
    "    )\n",
    "    \n",
    "    # Get the best split\n",
    "    best_split = info_gain.orderBy(F.desc(\"info_gain\")).first()\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    \n",
    "    # Prepare output DataFrame\n",
    "    if best_split is None:\n",
    "        result_df = spark.createDataFrame([(feature_index, float(0), float(0))], schema)\n",
    "    else:\n",
    "        result_df = spark.createDataFrame([(feature_index, float(best_split[\"split_value\"]), best_split[\"info_gain\"])], schema)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6e11fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tree from splitting\n",
    "\n",
    "# each tree\n",
    "# (i) for each feature: find_split\n",
    "# (ii) Mapbypartition(find_split)\n",
    "\n",
    "def feature_split(dataset, feature_array):\n",
    "    \n",
    "    ''' \n",
    "    Input: \n",
    "    partition: a pyspark dataframe partition to be called by foreachPartition,\n",
    "    feature_array: a broadcasted feature array for the tree that is intiialized earlier on\n",
    "    '''\n",
    "    #define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    feature_df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    # for each feature array, get a split and append the dataframe \n",
    "    for feature_index in feature_array:\n",
    "        \n",
    "        # find split\n",
    "        feature_split = new_split(dataset, feature_index)\n",
    "        \n",
    "        #add feature  \n",
    "        feature_df = feature_df.union(feature_split)\n",
    "        \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28730081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e896b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1fb1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_node(df,feature_array):\n",
    "\n",
    "    #init\n",
    "    y_label = df.columns[-1]\n",
    "    node_parent = {}\n",
    "    best_gain = -1\n",
    "    best_feature= 0 \n",
    "    best_split = 0\n",
    "    \n",
    "    #get first tree\n",
    "    feature_df = feature_split(df,feature_array)\n",
    "    feature_list = feature_df.rdd.collect()\n",
    "    \n",
    "    #init\n",
    "    for feature in feature_list:\n",
    "        \n",
    "        #assign val\n",
    "        feature_idx = feature[0]\n",
    "        split_val = feature[1]\n",
    "        gain = feature[2]\n",
    "        \n",
    "        if gain is None:\n",
    "            gain = 0\n",
    "        \n",
    "        #check best\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature_idx\n",
    "            best_split = split_val\n",
    "        \n",
    "    #generate split\n",
    "    left_df = df.filter(col(joined_df.columns[best_feature]) <= best_split)\n",
    "    right_df = df.filter(col(joined_df.columns[best_feature]) > best_split)\n",
    "        \n",
    "    #assign dictinary value of; key: feature, split_value, gain, and child\n",
    "    node_parent['feature'] = best_feature\n",
    "    node_parent['split_value'] = best_split\n",
    "    node_parent['gain'] = best_gain\n",
    "    \n",
    "    \n",
    "    return node_parent, left_df, right_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "008a9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decision_tree(df, feature_array, max_depth, current_depth=0):\n",
    "    \n",
    "    \"\"\" Recursively build the decision tree. \"\"\"\n",
    "    #set y label\n",
    "    label_col = df.columns[-1]\n",
    "    distinct_count = df.select(label_col).distinct().count()\n",
    "    \n",
    "    if current_depth == max_depth or distinct_count == 1 :\n",
    "        return df\n",
    "        # Return the most common label\n",
    "        most_common_label = df.groupBy(label_col).count().orderBy(\"count\", ascending=False).first()[label_col]\n",
    "        #return most_common_label\n",
    "    \n",
    "    #split node\n",
    "    node_parent, left_df, right_df = split_node(df,feature_array)\n",
    "    \n",
    "    #split if not empty\n",
    "    if left_df is None or left_df == []:\n",
    "        right_subtree = build_decision_tree(right_df, feature_array, max_depth, current_depth + 1)\n",
    "    elif right_df is None or right_df ==[]:\n",
    "        left_subtree = build_decision_tree(left_df, feature_array, max_depth, current_depth + 1)\n",
    "    elif (left_df is None) and (right_df is None):\n",
    "        return {\"feature\": node_parent['feature'] , \"threshold\": node_parent['split_value'], \"left\": None, \"right\": None}\n",
    "    else:\n",
    "        left_subtree = build_decision_tree(left_df, feature_array, max_depth, current_depth + 1)\n",
    "        right_subtree = build_decision_tree(right_df, feature_array, max_depth, current_depth + 1)\n",
    "\n",
    "    return {\"feature\": node_parent['feature'] , \"threshold\": node_parent['split_value'], \"left\": left_subtree, \"right\": right_subtree}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "373d9367",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parent, left_df, right_df = split_node(joined_df,[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "043ae424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Sampling fraction (1.25) must be on interval [0, 1] without replacement",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tree \u001b[38;5;241m=\u001b[39m build_decision_tree(joined_df, [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m], max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m tree\n",
      "Cell \u001b[0;32mIn[33], line 25\u001b[0m, in \u001b[0;36mbuild_decision_tree\u001b[0;34m(df, feature_array, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     left_subtree \u001b[38;5;241m=\u001b[39m build_decision_tree(left_df, feature_array, max_depth, current_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     right_subtree \u001b[38;5;241m=\u001b[39m build_decision_tree(right_df, feature_array, max_depth, current_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: left_subtree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m: right_subtree}\n",
      "Cell \u001b[0;32mIn[33], line 25\u001b[0m, in \u001b[0;36mbuild_decision_tree\u001b[0;34m(df, feature_array, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     left_subtree \u001b[38;5;241m=\u001b[39m build_decision_tree(left_df, feature_array, max_depth, current_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     right_subtree \u001b[38;5;241m=\u001b[39m build_decision_tree(right_df, feature_array, max_depth, current_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: left_subtree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m: right_subtree}\n",
      "Cell \u001b[0;32mIn[33], line 26\u001b[0m, in \u001b[0;36mbuild_decision_tree\u001b[0;34m(df, feature_array, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     left_subtree \u001b[38;5;241m=\u001b[39m build_decision_tree(left_df, feature_array, max_depth, current_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     right_subtree \u001b[38;5;241m=\u001b[39m build_decision_tree(right_df, feature_array, max_depth, current_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m'\u001b[39m] , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m\"\u001b[39m: node_parent[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: left_subtree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m: right_subtree}\n",
      "Cell \u001b[0;32mIn[33], line 15\u001b[0m, in \u001b[0;36mbuild_decision_tree\u001b[0;34m(df, feature_array, max_depth, current_depth)\u001b[0m\n\u001b[1;32m     11\u001b[0m     most_common_label \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupBy(label_col)\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mfirst()[label_col]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#return most_common_label\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#split node\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m node_parent, left_df, right_df \u001b[38;5;241m=\u001b[39m split_node(df,feature_array)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#split if not empty\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m left_df \u001b[38;5;241m==\u001b[39m []:\n",
      "Cell \u001b[0;32mIn[32], line 12\u001b[0m, in \u001b[0;36msplit_node\u001b[0;34m(df, feature_array)\u001b[0m\n\u001b[1;32m      9\u001b[0m best_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#get first tree\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m feature_df \u001b[38;5;241m=\u001b[39m feature_split(df,feature_array)\n\u001b[1;32m     13\u001b[0m feature_list \u001b[38;5;241m=\u001b[39m feature_df\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#init\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 26\u001b[0m, in \u001b[0;36mfeature_split\u001b[0;34m(dataset, feature_array)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# for each feature array, get a split and append the dataframe \u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_index \u001b[38;5;129;01min\u001b[39;00m feature_array:\n\u001b[1;32m     24\u001b[0m     \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# find split\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     feature_split \u001b[38;5;241m=\u001b[39m new_split(dataset, feature_index)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#add feature  \u001b[39;00m\n\u001b[1;32m     29\u001b[0m     feature_df \u001b[38;5;241m=\u001b[39m feature_df\u001b[38;5;241m.\u001b[39munion(feature_split)\n",
      "Cell \u001b[0;32mIn[30], line 18\u001b[0m, in \u001b[0;36mnew_split\u001b[0;34m(joined_df, feature_index)\u001b[0m\n\u001b[1;32m     12\u001b[0m parent_data_count \u001b[38;5;241m=\u001b[39m joined_df\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Calculate potential splits and their Information Gain\u001b[39;00m\n\u001b[1;32m     15\u001b[0m distinct_values \u001b[38;5;241m=\u001b[39m split_data\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumnRenamed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m/\u001b[39mparent_data_count)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Cartesian join to get split mask\u001b[39;00m\n\u001b[1;32m     21\u001b[0m splits_info \u001b[38;5;241m=\u001b[39m distinct_values\u001b[38;5;241m.\u001b[39mcrossJoin(split_data)\\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_left\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/HKUST/spark-3.5.0-bin-hadoop3/python/pyspark/sql/dataframe.py:1965\u001b[0m, in \u001b[0;36mDataFrame.sample\u001b[0;34m(self, withReplacement, fraction, seed)\u001b[0m\n\u001b[1;32m   1963\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(seed) \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m args \u001b[38;5;241m=\u001b[39m [arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m [withReplacement, fraction, seed] \u001b[38;5;28;01mif\u001b[39;00m arg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m-> 1965\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1966\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/Documents/HKUST/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/HKUST/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Sampling fraction (1.25) must be on interval [0, 1] without replacement"
     ]
    }
   ],
   "source": [
    "tree = build_decision_tree(joined_df, [0,2,3], max_depth=5)\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c9f8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_train(df, num_trees, sample_size, num_feature, max_depth):\n",
    "    #init\n",
    "    trees = []\n",
    "    feature_matrix = {}\n",
    "    \n",
    "    # Broadcast key parameters\n",
    "    max_depth_broadcast = sc.broadcast(max_depth)\n",
    "    \n",
    "    #set rf parameters\n",
    "    for i in range(num_trees):\n",
    "    \n",
    "        #sample features\n",
    "        feature_array = rand.sample(range(len(df.columns[:-1])),5)\n",
    "        feature_broadcast = sc.broadcast(feature_array)\n",
    "        \n",
    "        #sample df\n",
    "        sample_df = df.sample(True, sample_size)\n",
    "    \n",
    "        # Apply function on DataFrame\n",
    "        #foreach = df.foreachPartition(process_row)\n",
    "        tree = build_decision_tree(sample_df,feature_array,max_depth)\n",
    "        trees.append(tree)\n",
    "    \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca9885cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trees = random_forest_train(joined_df, num_trees=3, sample_size=0.5 ,num_feature = 3, max_depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "955a0d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'feature': 1,\n",
       "  'threshold': 0.11191961914300919,\n",
       "  'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "  'right': {'feature': 9,\n",
       "   'threshold': 0.11323804408311844,\n",
       "   'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "   'right': {'feature': 1,\n",
       "    'threshold': 0.665036678314209,\n",
       "    'left': {'feature': 9,\n",
       "     'threshold': 0.9338750243186951,\n",
       "     'left': {'feature': 9,\n",
       "      'threshold': 0.23754332959651947,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]},\n",
       "     'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]},\n",
       "    'right': {'feature': 9,\n",
       "     'threshold': 0.5968428254127502,\n",
       "     'left': {'feature': 1,\n",
       "      'threshold': 0.9691026210784912,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]},\n",
       "     'right': {'feature': 1,\n",
       "      'threshold': 0.9362122416496277,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]}}}}},\n",
       " {'feature': 7,\n",
       "  'threshold': 0.4799388349056244,\n",
       "  'left': {'feature': 2,\n",
       "   'threshold': 0.7709218263626099,\n",
       "   'left': {'feature': 2,\n",
       "    'threshold': 0.5519068241119385,\n",
       "    'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "    'right': {'feature': 6,\n",
       "     'threshold': 0.23877714574337006,\n",
       "     'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "     'right': {'feature': 4,\n",
       "      'threshold': 0.10178247094154358,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]}}},\n",
       "   'right': {'feature': 6,\n",
       "    'threshold': 0.7734731435775757,\n",
       "    'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "    'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]}},\n",
       "  'right': {'feature': 7,\n",
       "   'threshold': 0.6796472668647766,\n",
       "   'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "   'right': {'feature': 7,\n",
       "    'threshold': 0.7622851133346558,\n",
       "    'left': {'feature': 6,\n",
       "     'threshold': 0.3398749530315399,\n",
       "     'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "     'right': {'feature': 7,\n",
       "      'threshold': 0.7474709153175354,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]}},\n",
       "    'right': {'feature': 7,\n",
       "     'threshold': 0.7622851133346558,\n",
       "     'left': {'feature': 7,\n",
       "      'threshold': 0.0,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]},\n",
       "     'right': {'feature': 7,\n",
       "      'threshold': 0.7622851133346558,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]}}}}},\n",
       " {'feature': 3,\n",
       "  'threshold': 0.45522016286849976,\n",
       "  'left': {'feature': 4,\n",
       "   'threshold': 0.7972953915596008,\n",
       "   'left': {'feature': 2,\n",
       "    'threshold': 0.47157618403434753,\n",
       "    'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "    'right': {'feature': 7,\n",
       "     'threshold': 0.1104741096496582,\n",
       "     'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "     'right': {'feature': 8,\n",
       "      'threshold': 0.15436270833015442,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]}}},\n",
       "   'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]},\n",
       "  'right': {'feature': 2,\n",
       "   'threshold': 0.5868411064147949,\n",
       "   'left': {'feature': 2,\n",
       "    'threshold': 0.30952760577201843,\n",
       "    'left': {'feature': 3,\n",
       "     'threshold': 0.7439463138580322,\n",
       "     'left': {'feature': 3,\n",
       "      'threshold': 0.5369744300842285,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]},\n",
       "     'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]},\n",
       "    'right': {'feature': 8,\n",
       "     'threshold': 0.08162998408079147,\n",
       "     'left': {'feature': 7,\n",
       "      'threshold': 0.5074677467346191,\n",
       "      'left': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint],\n",
       "      'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]},\n",
       "     'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]}},\n",
       "   'right': DataFrame[_1: double, _2: double, _3: double, _4: double, _5: double, _6: double, _7: double, _8: double, _9: double, _10: double, y: bigint]}}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ca50d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8283e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6d921a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6214602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35c107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b17362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c7944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4bf2b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e852ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e4153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53acfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae070385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
