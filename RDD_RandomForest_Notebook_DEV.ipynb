{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2bdfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F, SparkSession\n",
    "import findspark \n",
    "import typing as t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2deb03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/04 23:06:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def init_spark(app_name=\"HelloWorldApp\", execution_mode=\"local[*]\"):\n",
    "    findspark.init()\n",
    "    spark = t.cast(\n",
    "        SparkSession,\n",
    "        SparkSession.builder.master(execution_mode).appName(app_name).getOrCreate(),\n",
    "    )\n",
    "    sc = spark.sparkContext\n",
    "    return spark, sc\n",
    "\n",
    "spark, sc = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b3a5ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_categorical_arr: x_all_categorical_arr.shape=(100, 10)\n",
      "all_numerical_arr: x_all_numerical_arr.shape=(100, 10)\n"
     ]
    }
   ],
   "source": [
    "#random gen dataset\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "x_all_categorical_arr = np.random.randint(0, 2, (100, 10))\n",
    "x_all_numerical_arr = np.random.rand(100, 10)\n",
    "y_categorical_arr = np.random.randint(0, 2, 100)\n",
    "#balanced_arr = np.concatenate([balanced_categorical_arr, balanced_numerical_arr], axis=1)\n",
    "print(f\"all_categorical_arr: {x_all_categorical_arr.shape=}\")\n",
    "print(f\"all_numerical_arr: {x_all_numerical_arr.shape=}\")\n",
    "#print(f\"balanced_arr: {balanced_arr.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67df864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = spark.createDataFrame(x_all_numerical_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3928a1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = spark.createDataFrame(y_categorical_arr,['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51014500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index\n",
    "x_indexed=x_data.withColumn(\"id\",monotonically_increasing_id())\n",
    "y_indexed=y_data.withColumn(\"id\",monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eef2fea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_1', '_2', '_3', '_4', '_5', '_6', '_7', '_8', '_9', '_10']\n",
      "['y']\n"
     ]
    }
   ],
   "source": [
    "# DEVELOPMENT: create joined df for computation with one id\n",
    "joined_df = x_indexed.join(y_indexed, \"id\").drop('id')\n",
    "x_train = joined_df.drop('y')\n",
    "y_train = joined_df.select('y')\n",
    "print(x_train.columns)\n",
    "print(y_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72908b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac62236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bootstrap function definition\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# weighted bootstrap subdataset\n",
    "\n",
    "#partition the dataframe dataset\n",
    "joined_df.repartition(10).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470df42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06a14e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap sampling per tree\n",
    "# create variations of the joined_df baseed on bootstramp algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f35d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boostrap data setsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00374565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8d937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce3767f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77d68bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entropy for classification evaluation crtieria\n",
    "# receives a probably as input to calculate entropy\n",
    "\n",
    "\n",
    "def class_entropy(df, total):\n",
    "    # Example entropy calculation for binary classification\n",
    "    col_name = \"y\"\n",
    "    counts = df.agg(F.count(col_name).alias(\"count\"))\n",
    "    return (\n",
    "        counts.withColumn(\"prob\", F.col(\"count\") / total)\n",
    "        .select(F.sum(-F.col(\"prob\") * F.log2(F.col(\"prob\"))).alias(\"entropy\"))\n",
    "        .first()[\"entropy\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def prob(df):\n",
    "    # Example entropy calculation for binary classification\n",
    "    count = np.count_nonzero(df)\n",
    "    if count == 0:\n",
    "        return float(0)\n",
    "    else:\n",
    "        total = len(df)\n",
    "        prob = np.divide(count, total)\n",
    "        return float(prob)\n",
    "\n",
    "\n",
    "class_entropy_udf = udf(class_entropy, ArrayType(DoubleType()))\n",
    "prob_udf = udf(prob, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a06a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcee16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import line_profiler\n",
    "import line_profiler\n",
    "import functools\n",
    "\n",
    "def profile_lines(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        profiler = line_profiler.LineProfiler()\n",
    "        profiler.add_function(func)\n",
    "        profiler.enable_by_count()\n",
    "        \n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        profiler.print_stats()\n",
    "        return result\n",
    "    \n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fd7d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile_lines\n",
    "def new_split(joined_df, feature_index):\n",
    "    \n",
    "    # Select relevant columns\n",
    "    feature_col_name = joined_df.columns[feature_index]\n",
    "    y_col_name = joined_df.columns[-1]\n",
    "    split_data = joined_df.select(feature_col_name, y_col_name)\\\n",
    "        .withColumnRenamed(feature_col_name,\"feature\")\\\n",
    "        .withColumnRenamed(y_col_name,\"y\")\n",
    "    \n",
    "    # Calculate parent entropy\n",
    "    parent_data_count = joined_df.count()\n",
    "    parent_entropy = class_entropy(joined_df.select(\"y\"), parent_data_count)\n",
    "    \n",
    "    # Calculate potential splits and their Information Gain\n",
    "    distinct_values = split_data.select(\"feature\")\\\n",
    "        .withColumnRenamed(\"feature\",\"split_value\")\\\n",
    "        .distinct().orderBy(\"split_value\")\n",
    "    \n",
    "    # Cartesian join to get split mask\n",
    "    splits_info = distinct_values.crossJoin(split_data)\\\n",
    "        .withColumn(\n",
    "        \"is_left\", F.col(\"feature\") <= F.col(\"split_value\")\n",
    "    )\n",
    "    \n",
    "    #aggregate list\n",
    "    entropies = splits_info.groupBy(\"split_value\", \"is_left\").agg(\n",
    "        F.count(\"y\").alias(\"count\"),\n",
    "        F.sum(\"y\").alias(\"sum\"),\n",
    "        prob_udf(F.collect_list(\"y\")).alias(\"prob\")\n",
    "    )\n",
    "    entropies = entropies.withColumn(\"entropy\",\\\n",
    "                                    -F.col(\"prob\") * F.log2(F.col(\"prob\")) \\\n",
    "                                    -(1-F.col(\"prob\")) * F.log2((1-F.col(\"prob\")))\n",
    "                                    )\n",
    "    # Calculate Information Gain for each split\n",
    "    info_gain = entropies.groupBy(\"split_value\").agg(\n",
    "        (parent_entropy - F.sum(F.col(\"entropy\") * (F.col(\"count\") / parent_data_count))).alias(\"info_gain\")\n",
    "    )\n",
    "    \n",
    "    # Get the best split\n",
    "    best_split = info_gain.orderBy(F.desc(\"info_gain\")).first()\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    \n",
    "    # Prepare output DataFrame\n",
    "    result_df = spark.createDataFrame([(feature_index, float(best_split[\"split_value\"]), best_split[\"info_gain\"])], schema)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "095c34c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 1.30635 s\n",
      "File: /var/folders/f3/y3235fxs3cl733b7qyjn10y80000gn/T/ipykernel_34469/503364529.py\n",
      "Function: new_split at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           @profile_lines\n",
      "     2                                           def new_split(joined_df, feature_index):\n",
      "     3                                               \n",
      "     4                                               # Select relevant columns\n",
      "     5         1      96000.0  96000.0      0.0      feature_col_name = joined_df.columns[feature_index]\n",
      "     6         1       2000.0   2000.0      0.0      y_col_name = joined_df.columns[-1]\n",
      "     7         2   16635000.0    8e+06      1.3      split_data = joined_df.select(feature_col_name, y_col_name)\\\n",
      "     8         1    1559000.0    2e+06      0.1          .withColumnRenamed(feature_col_name,\"feature\")\\\n",
      "     9         1     940000.0 940000.0      0.1          .withColumnRenamed(y_col_name,\"y\")\n",
      "    10                                               \n",
      "    11                                               # Calculate parent entropy\n",
      "    12         1  351490000.0    4e+08     26.9      parent_data_count = joined_df.count()\n",
      "    13         1  280361000.0    3e+08     21.5      parent_entropy = class_entropy(joined_df.select(\"y\"), parent_data_count)\n",
      "    14                                               \n",
      "    15                                               # Calculate potential splits and their Information Gain\n",
      "    16         2    4132000.0    2e+06      0.3      distinct_values = split_data.select(\"feature\")\\\n",
      "    17         1    1232000.0    1e+06      0.1          .withColumnRenamed(\"feature\",\"split_value\")\\\n",
      "    18         1    4458000.0    4e+06      0.3          .distinct().orderBy(\"split_value\")\n",
      "    19                                               \n",
      "    20                                               # Cartesian join to get split mask\n",
      "    21         2    2259000.0    1e+06      0.2      splits_info = distinct_values.crossJoin(split_data)\\\n",
      "    22         2    1443000.0 721500.0      0.1          .withColumn(\n",
      "    23         1    2964000.0    3e+06      0.2          \"is_left\", F.col(\"feature\") <= F.col(\"split_value\")\n",
      "    24                                               )\n",
      "    25                                               \n",
      "    26                                               #aggregate list\n",
      "    27         2   21284000.0    1e+07      1.6      entropies = splits_info.groupBy(\"split_value\", \"is_left\").agg(\n",
      "    28         1   12922000.0    1e+07      1.0          F.count(\"y\").alias(\"count\"),\n",
      "    29         1    2817000.0    3e+06      0.2          F.sum(\"y\").alias(\"sum\"),\n",
      "    30         1    4123000.0    4e+06      0.3          prob_udf(F.collect_list(\"y\")).alias(\"prob\")\n",
      "    31                                               )\n",
      "    32         2    2516000.0    1e+06      0.2      entropies = entropies.withColumn(\"entropy\",\\\n",
      "    33         2    5581000.0    3e+06      0.4                                      -F.col(\"prob\") * F.log2(F.col(\"prob\")) \\\n",
      "    34         1    5402000.0    5e+06      0.4                                      -(1-F.col(\"prob\")) * F.log2((1-F.col(\"prob\")))\n",
      "    35                                                                               )\n",
      "    36                                               # Calculate Information Gain for each split\n",
      "    37         2    5753000.0    3e+06      0.4      info_gain = entropies.groupBy(\"split_value\").agg(\n",
      "    38         1    4425000.0    4e+06      0.3          (parent_entropy - F.sum(F.col(\"entropy\") * (F.col(\"count\") / parent_data_count))).alias(\"info_gain\")\n",
      "    39                                               )\n",
      "    40                                               \n",
      "    41                                               # Get the best split\n",
      "    42         1  562570000.0    6e+08     43.1      best_split = info_gain.orderBy(F.desc(\"info_gain\")).first()\n",
      "    43                                               \n",
      "    44         2       7000.0   3500.0      0.0      schema = StructType([\n",
      "    45         1       2000.0   2000.0      0.0          StructField(\"feature\", IntegerType(), True),\n",
      "    46         1       2000.0   2000.0      0.0          StructField(\"split_value\", FloatType(), True),\n",
      "    47         1          0.0      0.0      0.0          StructField(\"info_gain\", FloatType(), True),\n",
      "    48                                               ])\n",
      "    49                                               \n",
      "    50                                               # Prepare output DataFrame\n",
      "    51         1   11378000.0    1e+07      0.9      result_df = spark.createDataFrame([(feature_index, float(best_split[\"split_value\"]), best_split[\"info_gain\"])], schema)\n",
      "    52                                               \n",
      "    53         1       1000.0   1000.0      0.0      return result_df\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[feature: int, split_value: float, info_gain: float]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_split(joined_df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fb0420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+\n",
      "|feature|split_value|  info_gain|\n",
      "+-------+-----------+-----------+\n",
      "|      0|0.057842676|0.021017073|\n",
      "+-------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = new_split(joined_df,0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6e11fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tree from splitting\n",
    "\n",
    "# each tree\n",
    "# (i) for each feature: find_split\n",
    "# (ii) Mapbypartition(find_split)\n",
    "\n",
    "def feature_split(dataset, feature_array):\n",
    "    \n",
    "    ''' \n",
    "    Input: \n",
    "    partition: a pyspark dataframe partition to be called by foreachPartition,\n",
    "    feature_array: a broadcasted feature array for the tree that is intiialized earlier on\n",
    "    '''\n",
    "    #define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    feature_df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    # for each feature array, get a split and append the dataframe \n",
    "    for feature_index in feature_array:\n",
    "        \n",
    "        # find split\n",
    "        feature_split = new_split(dataset, feature_index)\n",
    "        \n",
    "        #add feature  \n",
    "        feature_df = feature_df.union(feature_split)\n",
    "        \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f225c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = feature_split(joined_df,[0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f1fb1531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_tree(df,feature_array, max_depth=3):\n",
    "    \n",
    "\n",
    "    #init\n",
    "    y_label = df.columns[-1]\n",
    "    node = {}\n",
    "    np.zeros()\n",
    "\n",
    "    #get first tree\n",
    "    feature_df = feature_split(df,feature_array)\n",
    "    feature_list = feature_df.collect()\n",
    "    \n",
    "    #init\n",
    "    feature_idx = feature[0]\n",
    "    best_split = feature[1]\n",
    "    gain = feature[2]\n",
    "    \n",
    "\n",
    "    #generate split\n",
    "    left_df = df.filter(col(joined_df.columns[feature_idx]) <= best_split)\n",
    "    right_df = df.filter(col(joined_df.columns[feature_idx]) <= best_split)\n",
    "    \n",
    "    \n",
    "    return (feature_df, left_df,right_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3f6d921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math as m\n",
    "\n",
    "def random_forest_train(df, num_trees, max_depth=3):\n",
    "    trees = []\n",
    "    num_features = int(len(df.columns[:-1]))\n",
    "    \n",
    "    for _ in range(num_trees):\n",
    "        \n",
    "        #sample dataset with replacement\n",
    "        # to be replaced with sampling method from Jason\n",
    "        sampled_df = df.sample(withReplacement=True, fraction=1.0)\n",
    "        \n",
    "        # sample features \n",
    "        # to be replaced with a more updated version if available\n",
    "        feature_array = random.sample(range(num_features), k=int(m.log(num_features, 2) + 1))\n",
    "        \n",
    "        tree = grow_tree(sampled_df, feature_array, max_depth)\n",
    "        trees.append(tree)\n",
    "        \n",
    "        #create node\n",
    "        \n",
    "    return trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
