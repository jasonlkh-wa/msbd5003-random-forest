{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f132c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def prob(df):\n",
    "    #  probably calcluation for binary classification\n",
    "    count = np.count_nonzero(df)   \n",
    "    if count == 0:\n",
    "        return float(0)\n",
    "    else:\n",
    "        total = len(df)\n",
    "        prob = np.divide(count,total)\n",
    "        return float(prob)\n",
    "\n",
    "prob_udf = udf(prob,FloatType())\n",
    "\n",
    "OLD VERSION OF new_split, please refer to new_split_2\n",
    "\n",
    "\n",
    "def split_node(df,feature_array):\n",
    "\n",
    "    #init\n",
    "    y_label = df.columns[-1]\n",
    "    node_parent = {}\n",
    "    best_gain = -1\n",
    "    best_feature= 0 \n",
    "    best_split = 0\n",
    "    \n",
    "    #get first tree\n",
    "    feature_df = feature_split(df,feature_array)\n",
    "    feature_list = feature_df.rdd.collect()\n",
    "    \n",
    "    #init\n",
    "    for feature in feature_list:\n",
    "        \n",
    "        #assign val\n",
    "        feature_idx = feature[0]\n",
    "        split_val = feature[1]\n",
    "        gain = feature[2]\n",
    "        \n",
    "        if gain is None:\n",
    "            gain = 0\n",
    "        \n",
    "        #check best\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature_idx\n",
    "            best_split = split_val\n",
    "        \n",
    "    #generate split\n",
    "    left_df = df.filter(col(joined_df.columns[best_feature]) <= best_split)\n",
    "    right_df = df.filter(col(joined_df.columns[best_feature]) > best_split)\n",
    "        \n",
    "    #assign dictinary value of; key: feature, split_value, gain, and child\n",
    "    node_parent['feature'] = best_feature\n",
    "    node_parent['split_value'] = best_split\n",
    "    node_parent['gain'] = best_gain\n",
    "    \n",
    "    \n",
    "    return node_parent, left_df, right_df\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab087ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def build_decision_tree(df, feature_array, max_depth, current_depth=0):\n",
    "    \n",
    "    \"\"\" Recursively build the decision tree. \"\"\"\n",
    "    #set y label\n",
    "    label_col = df.columns[-1]\n",
    "    distinct_count = df.select(label_col).distinct().count()\n",
    "    \n",
    "    if df is None or df.rdd.isEmpty():\n",
    "        return df\n",
    "    elif current_depth == max_depth:\n",
    "        # Return the most common label\n",
    "        most_common_label = df.groupBy(label_col).count().orderBy(\"count\", ascending=False).first()[label_col]\n",
    "        return df\n",
    "        #return most_common_label \n",
    "    elif distinct_count == 1:\n",
    "        return {\"label\": df.select(label_col).first()[0]}\n",
    "    \n",
    "    #split node\n",
    "    node_parent, left_df, right_df = split_node_2(df,feature_array)\n",
    "    \n",
    "    \n",
    "    # Cache potentially reused DataFrames\n",
    "    if left_df is not None and not left_df.rdd.isEmpty():\n",
    "        left_df.cache()\n",
    "    if right_df is not None and not right_df.rdd.isEmpty():\n",
    "        right_df.cache()\n",
    "\n",
    "    \n",
    "    #split if not empty    \n",
    "    if left_df is None or left_df == []:\n",
    "        right_subtree = build_decision_tree(right_df, feature_array, max_depth, current_depth + 1)\n",
    "    elif right_df is None or right_df ==[]:\n",
    "        left_subtree = build_decision_tree(left_df, feature_array, max_depth, current_depth + 1)\n",
    "    elif (left_df is None) and (right_df is None):\n",
    "        return {\"feature\": node_parent['feature'] , \"threshold\": node_parent['split_value'], \"left\": None, \"right\": None}\n",
    "    else:\n",
    "        left_subtree = build_decision_tree(left_df, feature_array, max_depth, current_depth + 1)\n",
    "        right_subtree = build_decision_tree(right_df, feature_array, max_depth, current_depth + 1)\n",
    "\n",
    "    # Clear cache after use\n",
    "    if left_df is not None:\n",
    "        left_df.unpersist()\n",
    "    if right_df is not None:\n",
    "        right_df.unpersist()\n",
    "        \n",
    "    \n",
    "    # return node structure\n",
    "    return {\n",
    "        \"feature\": node_parent['feature'] , \n",
    "        \"threshold\": node_parent['split_value'], \n",
    "        \"left\": left_subtree, \n",
    "        \"right\": right_subtree\n",
    "    }\n",
    "\n",
    "#TESTING:\n",
    "tree = build_decision_tree(joined_df, [0,2,3], max_depth=5)\n",
    "tree\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def feature_split_2(dataset, feature_array):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - dataset: a pyspark dataframe partition to be called by foreachPartition,\n",
    "    - feature_array: a broadcasted feature array for the tree that is initialized earlier on\n",
    "    \"\"\"\n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    \n",
    "    # Broadcast feature array\n",
    "    broadcast_feature_array = spark.sparkContext.broadcast(feature_array)\n",
    "    \n",
    "    # Register the DataFrame as a temporary view\n",
    "    dataset.createOrReplaceTempView(\"dataset\")\n",
    "    \n",
    "    # Create an empty DataFrame with the defined schema\n",
    "    result_df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    for feature_index in broadcast_feature_array.value:\n",
    "        feature_col_name = dataset.columns[feature_index]\n",
    "        \n",
    "        # Use SQL to find split for the feature\n",
    "        split_result = spark.sql(f\"\"\"\n",
    "            WITH distinct_values AS (\n",
    "                SELECT DISTINCT {feature_col_name} AS split_value\n",
    "                FROM dataset\n",
    "                ORDER BY RAND()\n",
    "                LIMIT 10\n",
    "            ),\n",
    "            left_counts AS (\n",
    "                SELECT split_value, COUNT(*) AS left_count, SUM(y) AS left_sum\n",
    "                FROM dataset, distinct_values\n",
    "                WHERE {feature_col_name} <= split_value\n",
    "                GROUP BY split_value\n",
    "            ),\n",
    "            right_counts AS (\n",
    "                SELECT split_value, COUNT(*) AS right_count, SUM(y) AS right_sum\n",
    "                FROM dataset, distinct_values\n",
    "                WHERE {feature_col_name} > split_value\n",
    "                GROUP BY split_value\n",
    "            )\n",
    "            SELECT\n",
    "                {feature_index} AS feature,\n",
    "                d.split_value,\n",
    "                (left_counts.left_count / (left_counts.left_count + right_counts.right_count)) * (\n",
    "                    -(left_counts.left_sum / left_counts.left_count) * LOG2(left_counts.left_sum / left_counts.left_count) - \n",
    "                    (1 - left_counts.left_sum / left_counts.left_count) * LOG2(1 - left_counts.left_sum / left_counts.left_count)\n",
    "                ) +\n",
    "                (right_counts.right_count / (left_counts.left_count + right_counts.right_count)) * (\n",
    "                    -(right_counts.right_sum / right_counts.right_count) * LOG2(right_counts.right_sum / right_counts.right_count) - \n",
    "                    (1 - right_counts.right_sum / right_counts.right_count) * LOG2(1 - right_counts.right_sum / right_counts.right_count)\n",
    "                ) AS info_gain\n",
    "            FROM distinct_values d\n",
    "            JOIN left_counts ON d.split_value = left_counts.split_value\n",
    "            JOIN right_counts ON d.split_value = right_counts.split_value\n",
    "            ORDER BY info_gain DESC\n",
    "            LIMIT 1\n",
    "        \"\"\")\n",
    "        \n",
    "        # Append the split result to the result DataFrame\n",
    "        result_df = result_df.union(split_result.withColumn(\"feature\", F.lit(feature_index)))\n",
    "    \n",
    "    return result_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Build tree from splitting\n",
    "\n",
    "# each tree\n",
    "# (i) for each feature: find_split\n",
    "# (ii) Mapbypartition(find_split)\n",
    "\n",
    "def feature_split(dataset, feature_array):\n",
    "    \n",
    "\n",
    "    Input: \n",
    "    partition: a pyspark dataframe partition to be called by foreachPartition,\n",
    "    feature_array: a broadcasted feature array for the tree that is intiialized earlier on\n",
    "    #define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"feature\", IntegerType(), True),\n",
    "        StructField(\"split_value\", FloatType(), True),\n",
    "        StructField(\"info_gain\", FloatType(), True),\n",
    "    ])\n",
    "    feature_df = spark.createDataFrame([], schema)\n",
    "    \n",
    "    # for each feature array, get a split and append the dataframe \n",
    "    for feature_index in feature_array:\n",
    "        \n",
    "        # find split\n",
    "        feature_split = new_split_2(dataset, feature_index)\n",
    "        \n",
    "        #add feature  \n",
    "        feature_df = feature_df.union(feature_split)\n",
    "        \n",
    "    return feature_df\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
